
import subprocess
import pandas as pd                     # for working with dataframes
from datetime import date               # for obtaining current date
import json
import gc
import requests
import re
from time import sleep


# define filenames
CVE_reporter_report_name = "vulnerability-report-kaptain-main.csv"          # will read this file as input - initial report
Resulting_report_name = f"grype-report-kaptain-{date.today()}.csv"          # will create this file as output - final report
grype_avail_cols_file_name = "grype-fields-list.txt"                       # will also create this file as output - list of grype avail fields


# function for filling the vulnerability_CVE and vulnerability_GHSA columns with corresponding GHSA and CVE IDs
def fillCVEandGHSA(df, vuln_col_name):

    # create empty columns vulnerability_CVE and vulnerability_GHSA
    df['vulnerabiligy_CVE'] = None
    df['vulnerability_GHSA'] = None

    # add available values to both columns
    df.loc[df[vuln_col_name].str.contains(r'(CVE)'), 'vulnerabiligy_CVE'] = df[vuln_col_name].loc[df[vuln_col_name].str.contains(r'(CVE)')]
    df.loc[df[vuln_col_name].str.contains(r'(GHSA)'), 'vulnerability_GHSA'] = df[vuln_col_name].loc[df[vuln_col_name].str.contains(r'(GHSA)')]
    
    # drop original column
    df.drop(columns=vuln_col_name, inplace=True)

    for index, row in df[:5].iterrows():
        vuln_CVE_val = row['vulnerabiligy_CVE']        
        vuln_GHSA_val = row['vulnerability_GHSA']
        CVE_regex_pattern = 'CVE-[0-9-]*'
        GHSA_regex_pattern = 'GHSA-[a-z0-9-]*'    

        def get_ID(regex_pat, url):
            ID_string = None

            # aparently github blocks connection after too many attempts
            waiting_time = 1
            done = False
            while done == False:
                try:
                    r = requests.get(url, allow_redirects=True)
                    done = True
                except:
                    waiting_time += 3
                    print('github connection was reset.  Waiting', waiting_time, 'seconds')
                    sleep(waiting_time)
            ID_string =  re.findall(CVE_regex_pattern, r.text)
            if len(ID_string) > 0:
                ID_string = ID_string[0]
            else:
                ID_string = None
            return ID_string    

        if vuln_CVE_val is None:
            url = 'https://github.com/advisories/' + vuln_GHSA_val
            df.at[index,'vulnerabiligy_CVE'] = get_ID(CVE_regex_pattern, url)

        elif vuln_GHSA_val is None:
            url = 'https://github.com/advisories?query=' + vuln_CVE_val
            df.at[index,'vulnerabiligy_GHSA'] = get_ID(GHSA_regex_pattern, url)
    return df


print('starting...')

# Load CVE reporter report 
CVE_Reporter_report_df = pd.read_csv(CVE_reporter_report_name, header=0)

# Filter CVE reporter report by 'critical' CVEs only
CVE_Reporter_report_df = CVE_Reporter_report_df[CVE_Reporter_report_df['vulnerability_severity'] == 'critical']

# Discard unnecesary columns from CVE reporter report
CVE_Reporter_report_df = CVE_Reporter_report_df[['resource_purl', 'vulnerability_name', 'mitigation_url', 'vulnerability_severity']]

# add columns vulnerabiligy_CVE and vulnerability_GHSA with corresponding GHSA and CVE IDs
print('fetching some GHSAs and CVEs from github for the CVE reporter report')
CVE_Reporter_report_df = fillCVEandGHSA(df=CVE_Reporter_report_df, vuln_col_name='vulnerability_name')

# Create separate series of unique CVE reporter images for iteration
series_of_CVE_reporter_images = pd.Series(CVE_Reporter_report_df['resource_purl'])
series_of_CVE_reporter_images = pd.Series(series_of_CVE_reporter_images.unique())
series_of_CVE_reporter_images = series_of_CVE_reporter_images[~series_of_CVE_reporter_images.str.contains('mesosphere/kubeflow-dev')]
series_of_CVE_reporter_images = series_of_CVE_reporter_images.reset_index(drop=True)

# Create a separate set for available columns from grype
grype_avail_cols = set()

print("Images to scan:", len(series_of_CVE_reporter_images))

# specify needed columns from grype_results_df
Needed_columns = [
    'image.name',
    'resource_purl',
    'vulnerability.id', 
    'vulnerability.dataSource', 
    'artifact.purl', 
    'vulnerability.fix.state', 
    'vulnerability.description', 
    'artifact.metadata.virtualPath',
    'vulnerability.fix.versions'
    ]

# Create dataframe for storing the final results
grype_results_df = pd.DataFrame(columns=Needed_columns)

for index, image in series_of_CVE_reporter_images.items():

    print("Image", index + 1, image)

    # Create dataframe to store results for current image
    Filtered_img_CVEs_df = pd.DataFrame()

    # Format image string
    Pullable_image_name = image.replace("pkg:docker/", "")
    Pullable_image_name = Pullable_image_name.replace("@", ":")
    regex = "?repository_url="
    Split_parts = Pullable_image_name.split(regex)
    if len(Split_parts) > 1:
        Pullable_image_name = Split_parts[1] + "/" + Split_parts[0]
    else:
        Pullable_image_name = Split_parts[0]

    # Run grype scan on image
    Grype_run = subprocess.run(["./grype-scan.sh", Pullable_image_name.strip()], capture_output=True)


    # Load scan results in dataframe
    Current_img_CVEs_json = json.loads(Grype_run.stdout.decode('utf8'))
    Current_img_CVEs_df = pd.json_normalize(Current_img_CVEs_json['matches'])


    # Add all available columns from the dataframe to the available columns:
    grype_avail_cols.update(Current_img_CVEs_df.columns)    
    
    
    # Add the image name and resource purl to the results
    Current_img_CVEs_df['image.name'] = Pullable_image_name
    Current_img_CVEs_df['resource_purl'] = image


    # extract needed columns from grype's report
    for col in Current_img_CVEs_df:
        for needed_col in Needed_columns:
            if needed_col in Current_img_CVEs_df.columns:
                Filtered_img_CVEs_df[needed_col] = Current_img_CVEs_df[needed_col]


    # Add image results to final results
    grype_results_df = pd.concat([grype_results_df, Filtered_img_CVEs_df], ignore_index=True)


    gc.collect()


# add columns vulnerabiligy_CVE and vulnerability_GHSA with corresponding GHSA and CVE IDs
print('fetching some GHSAs and CVEs from github for the grype report')
grype_results_df = fillCVEandGHSA(df=grype_results_df, vuln_col_name='vulnerability.id')

# create 2 copies of grype_results_df for the two type of mergers
grype_df_vulnerabiligy_CVE = grype_results_df.drop(columns='vulnerability_GHSA')
grype_df_vulnerability_GHSA = grype_results_df.drop(columns='vulnerabiligy_CVE')
grype_df_vulnerabiligy_CVE=grype_df_vulnerabiligy_CVE[grype_df_vulnerabiligy_CVE.vulnerabiligy_CVE.notnull()]
grype_df_vulnerability_GHSA=grype_df_vulnerability_GHSA[grype_df_vulnerability_GHSA.vulnerability_GHSA.notnull()]

# merge 1 of 2: between CVE reporter and grype report using columns resource_purl and vulnerabiligy_CVE
if len(grype_df_vulnerabiligy_CVE) > 0:

    CVE_merge_df = pd.merge(
        CVE_Reporter_report_df, 
        grype_df_vulnerabiligy_CVE, 
        how='left', 
        left_on=['resource_purl', 'vulnerabiligy_CVE'],
        right_on=['resource_purl', 'vulnerabiligy_CVE'],
        suffixes=("_left", "_right"),
        indicator=True)

    CVE_merge_leftover_df = CVE_merge_df[CVE_merge_df['_merge'] == 'left_only']

    # merge 2 of 2: between records not matched in 1st merge and grype report using columns resource_purl and vulnerability_GHSA
    if len(CVE_merge_leftover_df) > 0 and len(grype_df_vulnerability_GHSA) > 0:
        CVE_merge_leftover_df.drop(columns='_merge', inplace=True)

        GHSA_merge_df = pd.merge(
            CVE_merge_leftover_df, 
            grype_df_vulnerability_GHSA,
            how='left', 
            left_on=['resource_purl', 'vulnerability_GHSA'],
            right_on=['resource_purl', 'vulnerability_GHSA'],
            suffixes=("_left", "_right"),
            indicator=True)
        
        # remove results only present in grype
        GHSA_merge_df = GHSA_merge_df[GHSA_merge_df['_merge'] != 'right_only']

        # concatenate both dataframes to form final results
        Final_results_df = pd.concat([CVE_merge_df, GHSA_merge_df], ignore_index=True)

    else:
        Final_results_df = CVE_merge_df[CVE_merge_df['_merge'] != 'right_only']



# final adjustments:
# split artifact.purl column into affected.artifact and artifact.version columns
Final_results_df[['affected.artifact', 'artifact.version']] = Final_results_df['artifact.purl'].str.split('@', expand=True)

# remove garbage before the last "/" from affected.artifact column
regexp = ".+/"
Final_results_df['affected.artifact'] = Final_results_df['affected.artifact'].str.replace(regexp, '', regex=True)

# remove garbage after the first "?" from the artifact.version column
regexp = "\?.+"
Final_results_df['artifact.version'] = Final_results_df['artifact.version'].str.replace(regexp, '', regex=True)

# clean columns before saving
Final_results_df["vulnerability.fix.versions"] = Final_results_df["vulnerability.fix.versions"].str.replace("[]", '', regex=False)

Final_column_order = [
    'resource_purl',
    'vulnerabiligy_CVE',
    'vulnerability_GHSA',
    'image.name',
    'mitigation_url',
    'vulnerability.dataSource',
    'vulnerability_severity',
    'vulnerability.fix.state',
    'vulnerability.description',
    'affected.artifact',
    'artifact.version',
    'artifact.metadata.virtualPath',
    'vulnerability.fix.versions',
    'artifact.purl',
    'grype'
    ]


# rename the _merge column for grype
Final_results_df.rename(columns = {'_merge':'grype'}, inplace = True)

# rename values in the new grype column: {'left_only': 'no', 'both': 'yes'}
Final_results_df["grype"].replace({"left_only": "no", "both": "yes"}, inplace=True)

# save CVE report results
Final_results_df[Final_column_order].to_csv(Resulting_report_name, index=False)

# save list of available fields from grype too
grype_avail_cols_list = sorted(grype_avail_cols)
with open('grype-fields-list.txt', 'w') as f:
    for item in grype_avail_cols_list:
        f.write("%s\n" % item)